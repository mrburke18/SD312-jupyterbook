# Value Function Approximation to Modern Q-Learning

Previously in AI, you learned about *value functions* in reinforcement learning.  Just to square our notation, I'll remind you that the value of a state is equal to the expected sum of rewards given a policy.  That is, we have a *policy* $\pi(s)$, which, for every state $s$, returns an action $a$.  This policy defines our behavior.  This could be a good policy, or a bad policy, but it's a policy.  We can say that given our state, we expect a certain reward now, then, following our policy, we'll end up at some state $s'$, where we can expect another reward.  From there, we can expect another reward, and so on.  We discount these out into the future using a discount factor $\gamma$.  So, $V_\pi(s)=R(s)+\gamma R(s')+\gamma^2 R(s'')...$.  We can (and usually do) also write this recursively, as $V_\pi(s)=R(s)+\gamma V(s')$.

In AI, you learned how to solve these for small domains using *value iteration*, where you start with an arbitrary guess of $V(s)$ for every $s$, and then apply the Bellman operator to refine that guess, and repeat and repeat and repeat, until things stop changing too much.  This required a table of approximate values for every state in the space.

What can be done, though, if the number of states is very large or infinite?  Then, we can't make such a table.  Well, think back to regression.  There, we assumed an infinite number of points, from which we had sampled a finite few.  From that finite few, we created a function, which we believed represented the pattern through the entirety of possible points.  We do the same thing in reinforcement learning.  We have a few samples $(s,a,r,s')$, where $s$ and $s'$ are states, $a$ is the action taken at state $s$, and $r$ is the reward received.  We'll create an approximate function which predicts $V(s)$ accurately for those points, and assume it's accurate for other points, too.

Of course, to do this, we need a loss function.  In regression, we usually use mean squared error, where we compare the target and the predicted target together.  In RL, we're predicting a value (call it $\hat V(s)$), but can't actually sample the value (you'll notice $V$ is not part of our $(s,a,r,s')$ sample), so we can't do that.  However, we could compare our prediction $\hat V(s)$ to what actually happened.  By this, I mean that we predict $\hat V(s)$ and $\hat V(s')$ for each of our samples.  If these were exactly accurate, then for every sample, $\hat V(s)=r+\gamma \hat V(s')$.  Of course, our prediction is probably not exactly accurate.  The difference between the left and right sides of this equality is the $Bellman error$.  We know our approximation is good if the magnitude of our Bellman error is small for all points.

So, we create a $\hat V$ that minimizes the Bellman error for our samples.  How do we create this?  Well, the same ways we might for regression.  We could use a linear approximation, where $\hat V(s)=\Phi(s)w$.  Or, we could use a kernelized approximation.  Or, we could use a neural network, where a state is input, and the value is output.  For all of these options, we choose our parameters based on what minimizes that Bellman error.

So, what's the point of this?  Well, the value function you just approximated is specific to the policy.  For example, suppose we have two actions, one which kills us, and one which gives us a cookie.  If your policy is to always choose the death option, then your value is going to be pretty low!  A value function is useful in that helps you improve your policy.  We know a policy is good if its value is high.  So, value function approximation is frequently part of *policy iteration*, where you generate a random policy, then collect samples and create a value function of that policy, then use that value function to improve that policy (oh, here I chose option A, and ended up in this state with low value, but if I'd chosen option B, I'd have ended up in that other state, which has a high value), then repeat.  This process is made easier with *Q-functions*, $Q(s,a)$, which are the value of taking a specific action at each state.  So now, a state and an action are fed into the function, and a single scalar value is predicted.  Again, we minimize Bellman error.

An accurate Q-function approximation allows an agent to say, "at this state, I could choose either of these two actions, so I'll calculate $\hat Q(s,a_1)$ and $\hat Q(s,a_2)$, and whichever is higher, is the action I'll take."  This approach was used in [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), the first paper to make a significant splash with deep learning and reinforcement learning.  In it, an agent learned to play Atari games, some at a superhuman level, by feeding an image of the game and an action into a convolutional neural net, which would output an approximate Q-value.

The drawbacks of Q-learning come in when the action space is large or continuous.  If we are trying to teach a robot to run, for example, an action consists of choosing behaviors at a number of joints simultaneously - we can't reasonably try all actions to see which is best.  We'll see how to address this on Wednesday.
