{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "046183a1-3eae-4d1c-8559-b28a427d73aa",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "We're going to use Deep Q-Learning in order to learn a [cartpole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) agent.  You'll notice the cartpole state space is continuous... Tabular Q-Learning won't work!\n",
    "\n",
    "Let's start by setting up our environment.  Run the three cells below to install gymnasium on AWS, retrieve a `.npy` file of states, import everything relevant, and see what the observations look like.\n",
    "\n",
    "After you run the first cell, I suggest commenting it out as you won't need to run it more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d71de19-7c3c-4f8f-9811-99b4d907e493",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gymnasium) (4.10.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from gymnasium[classic_control]) (2.5.2)\n",
      "--2024-04-30 01:08:41--  https://www.usna.edu/Users/cs/SD312/lab/13DeepQ/sampled_states.npy\n",
      "Resolving www.usna.edu (www.usna.edu)... 136.160.88.139\n",
      "Connecting to www.usna.edu (www.usna.edu)|136.160.88.139|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 928\n",
      "Saving to: ‘sampled_states.npy.4’\n",
      "\n",
      "100%[======================================>] 928         --.-K/s   in 0s      \n",
      "\n",
      "2024-04-30 01:08:41 (184 MB/s) - ‘sampled_states.npy.4’ saved [928/928]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "!pip install gymnasium gymnasium[classic_control]\n",
    "!wget https://www.usna.edu/Users/cs/SD312/lab/13DeepQ/sampled_states.npy\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb26dec3-3dec-480b-9494-cf03c765b0ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73af61e-b1b3-4dc2-a7f7-42ca085af3c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space is continuous in 4 dimensions, and there are 2 actions.\n",
      "For example, heres an observation: [ 0.03826823 -0.01193796 -0.03517876 -0.03836181].\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "STATE_DIM = env.observation_space.shape[0]\n",
    "N_ACTIONS = env.action_space.n\n",
    "GAMMA = .99\n",
    "\n",
    "print(f'State space is continuous in {STATE_DIM} dimensions, and there are {N_ACTIONS} actions.')\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(f'For example, heres an observation: {obs}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a73682-fe6a-4533-9871-6dfa545cb963",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "We have to decide a few things.\n",
    "- What should $\\epsilon$ be for our $\\epsilon$-greedy exploration policy?\n",
    "- How large should our replay be?\n",
    "- How many datapoints should we pull from our replay to train on at a time (batch size)?\n",
    "- What should our neural network look like? A good first step here is to make sure you understand what the dimensions of the input and output layers need to be - those aren't up to us, they are prescribed by the problem.\n",
    "\n",
    "Choose some values, design your network.\n",
    "\n",
    "Below we've also created a Replay - note that it's essentially a `deque` of limited size.  Make sure you understand that code!  In that replay, we are storing Transitions, each of which consists of a state, an action, a reward, and a next_state.  If the transition represents a failure (pole fell over or cart went off screen), the next_state will be `None`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0746df-448d-4194-94ba-738cabef4b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPSILON = 0.3\n",
    "REPLAY_LENGTH = 10000\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b6e896-bb5a-4f99-b959-3582bfd63f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a network and create an instance of it\n",
    "class Cartpole(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cartpole, self).__init__()\n",
    "        self.hidden = nn.Linear(4, 100) # input dimensionality\n",
    "        self.hidden_act = nn.ReLU()\n",
    "        self.output = nn.Linear(100, 2)  # output dimensionality\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.hidden_act(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be635599-ed61-4fbf-9b2e-7cb6c142e3cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'next_state'))\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, s, a, r, sp):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        s = torch.tensor(s, dtype=torch.float32)\n",
    "        a = torch.tensor([a], dtype=torch.int64)\n",
    "        r = torch.tensor([r], dtype=torch.float32)\n",
    "        if sp is not None:\n",
    "            sp = torch.tensor(sp, dtype=torch.float32)\n",
    "        self.memory.append(Transition(s,a,r,sp))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "memory=ReplayMemory(REPLAY_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd8295-89f0-4c5a-b1e6-31ea14c62b24",
   "metadata": {},
   "source": [
    "## Policies!\n",
    "\n",
    "The below functions define a greedy policy, an $\\epsilon$-greedy policy, and a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786427bb-4ed5-48f2-bf1a-6b44157d16c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def greedy_policy(network, states):\n",
    "    '''\n",
    "    Returns a tuple\n",
    "    index 0 contains the Q-value of the best action for all states\n",
    "    index 1 contains the index of the best action for all states\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        qs = network(states) # Get the q-values\n",
    "        if qs.dim() == 1:   # If it's just a single state\n",
    "            return torch.max(qs, dim=0) # Return the tuple of max info for that state\n",
    "        return torch.max(qs, dim=1) # Return the tuple of max information for all states\n",
    "\n",
    "def epsilon_greedy(network, state):\n",
    "    '''\n",
    "    Returns an action selected via epsilon-greedy\n",
    "    '''\n",
    "    if numpy.random.random() < EPSILON:\n",
    "        return numpy.random.randint(N_ACTIONS)\n",
    "    else:\n",
    "        return greedy_policy(network, torch.tensor(state).to('cuda'))[1].item()\n",
    "\n",
    "def random_policy():\n",
    "    '''\n",
    "    Chooses a random action.\n",
    "    '''\n",
    "    return numpy.random.randint(N_ACTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d230fe-2ce5-4aff-b753-2ca84d70dca1",
   "metadata": {},
   "source": [
    "## Random performance\n",
    "\n",
    "100 times, reset the environment and run it until truncation or termination, using a random policy.  Print out the average number of steps a random policy keeps the pole upright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3924a425-9df7-4b16-9b3e-8a67799d11f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.37\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "totalsteps = 0\n",
    "\n",
    "for i in range (100):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not terminated and not truncated:\n",
    "        action = random_policy()\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        steps += 1\n",
    "        \n",
    "    totalsteps += steps\n",
    "avg = totalsteps/100\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bf29a2-353b-47c1-9a08-109c9cb8541a",
   "metadata": {},
   "source": [
    "## Evaluating training\n",
    "\n",
    "We're going to reproduce the graphs on the right-hand side of Figure 2 in the paper in order to judge the smoothness of our training.  Create a function called `avg_qs`, which accepts as arguments your network and a tensor representing a group of states, which you should load from `sampled_states.npy`.  It should then do the following:\n",
    "\n",
    "- in a `with torch.no_grad()` block, push the states through the network, producing some approximate Q-values\n",
    "- calculate the maximum Q-value for each state\n",
    "- average those maximum Q-values over all the states, resulting in a single scalar\n",
    "- return that scalar (which should just be a number, not a tensor or numpy array - recall you can pull out the value from a tensor by using `.item()`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8641e204-5025-447b-9dc9-f60737c676c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03301790729165077"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_states = torch.tensor(np.load('sampled_states.npy'), dtype=torch.float32).to('cuda')\n",
    "cartpolenn = Cartpole().to('cuda')\n",
    "\n",
    "def avg_qs(network, states):\n",
    "    with torch.no_grad():\n",
    "        bestq, index = greedy_policy(network, states)\n",
    "        avgbestq = torch.mean(bestq).item()\n",
    "        \n",
    "    return avgbestq\n",
    "\n",
    "avg_qs(cartpolenn, sampled_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12154488-4bde-43b4-9140-7c9184373620",
   "metadata": {},
   "source": [
    "## Training your network\n",
    "\n",
    "- Create an optimizer\n",
    "- Choose a criterion\n",
    "- Understand, then complete, this `train_model()` function, which implements the steps in Algorithm 1 from the word \"Sample\" to the mention of equation 3.\n",
    "\n",
    "The initial steps in `train_model()` pull out the states, actions, and rewards from the batch, and turn them into cuda torch tensors.  `actual_next_mask` is the indices of the transitions in the batch that actually have a next state (ie, they don't represent a failure state, or in the parlance of the paper, are \"non-terminal\").  `next_states` is the torch tensor of all the next states for the non-terminal transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16a7b213-f391-4b07-a141-0aa3d521098b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(cartpolenn.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # Tensors of the states, actions, and rewards from the minibatch\n",
    "    # states is BATCH_SIZEx4\n",
    "    # actions is BATCH_SIZEx1\n",
    "    # rewards is BATCH_SIZEx1\n",
    "    states = torch.cat([transition.state.unsqueeze(0) for transition in transitions], dim=0).to('cuda')\n",
    "    actions = torch.cat([transition.action.unsqueeze(0) for transition in transitions], dim=0).to('cuda')\n",
    "    rewards = torch.cat([transition.reward.unsqueeze(0) for transition in transitions], dim=0).to('cuda')\n",
    "\n",
    "    # actual_next_mask contains the indices of samples without None next_states\n",
    "    actual_next_mask = [i for i in range(BATCH_SIZE) if transitions[i].next_state is not None]\n",
    "    # next_states contains those actual next_states\n",
    "    next_states = torch.cat([transition.next_state.unsqueeze(0) for transition in transitions if transition.next_state is not None], dim=0).to('cuda')\n",
    "    # the number of rows in `next_states` is the number of non-terminal states\n",
    "    # the number of elements in `actual_next_mask`\n",
    "    \n",
    "    preds = cartpolenn(states).gather(1, actions)\n",
    "    y = rewards\n",
    "    y[actual_next_mask] += GAMMA * greedy_policy(cartpolenn, next_states)[0]\n",
    "        \n",
    "    loss = criterion(preds, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3194c793-d25f-46fd-812c-862e194a851c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa384956-cf61-4a5c-a27e-d286d88ef885",
   "metadata": {},
   "source": [
    "## Create your samples, and call the training function\n",
    "\n",
    "Implement the rest of Algorithm 1, calling your `train_model()` function where appropriate.\n",
    "\n",
    "If the transition is terminal, the next_state should be `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "936d4085-cda3-4a91-894b-ff46ad9697ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [467, 1] doesn't match the broadcast shape [467, 467]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m action \u001b[38;5;241m=\u001b[39m epsilon_greedy(cartpolenn, state)\n\u001b[1;32m      8\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m preds \u001b[38;5;241m=\u001b[39m cartpolenn(states)\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions)\n\u001b[1;32m     25\u001b[0m y \u001b[38;5;241m=\u001b[39m rewards\n\u001b[0;32m---> 26\u001b[0m y[actual_next_mask] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m GAMMA \u001b[38;5;241m*\u001b[39m greedy_policy(cartpolenn, next_states)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(preds, y)\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [467, 1] doesn't match the broadcast shape [467, 467]"
     ]
    }
   ],
   "source": [
    "# generate a vector of q and y-values then minimize the difference\n",
    "for i in range(200):\n",
    "    state, info = env.reset()\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    while not truncated or terminated:\n",
    "        action = epsilon_greedy(cartpolenn, state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "        train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0afce-39ef-4cdd-b50e-4899c41c2571",
   "metadata": {},
   "source": [
    "## Evaluating the smoothness of your training\n",
    "\n",
    "In the above training loop, keep track of the average maximum Q values for the `sampled_states` you loaded above.  Make a plot displaying the average Q values of the sampled states over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4319889-bc22-40bb-b85c-2b7835cf4b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data = go.Scatter(x=list(range(len(qs))), y=qs, mode='lines'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c003fb6-e040-4817-8076-303b52c9a742",
   "metadata": {},
   "source": [
    "## Evaluating your model's performance\n",
    "\n",
    "1000 times, use a greedy policy based off your model to run until termination or truncation.  Keep track of the number of steps that pass on each run before it stops (each trial will run a maximum of 500 steps before truncation - of course, it may terminate sooner if the pole falls or the cart goes off the screen).  Print out that average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c75b71e-17fb-49f8-b30a-b39292c9bf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
