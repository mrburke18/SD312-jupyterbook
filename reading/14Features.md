**Read LFD 1.3**

- Understand 1.3.1, and the idea that we cannot definitively guarantee good performance on data we have not seen before.
- Think about equation 1.4.  Be sure to understand the meeting of all the terms.  What do we do to $\epsilon$ to make the probability large?  Small?  What about $N$?  What does that mean?  Be able to intuitively explain that inequality.
- How do the definitions of $E_{in}$ and $E_{out}$ get us to equation 1.5?
- Why does choosing a "good" hypothesis on $E_{in}$ break the Hoeffding inequality?  How does that lead to the inclusion of $M$ in 1.6?
- The box on page 26 is really important.
- Think about the "complexity" of a model.  What hyperparameter in our matrix completion project more complex or less complex?  What do we think that would mean about overfitting?
