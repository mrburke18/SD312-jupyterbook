**Read: RL 13.0-13.1**

In the parlance of this book, Q-Learning is an *action-value* method, in that it gives you the value of taking an action at a particular state: $Q(s,a)$.  What is the difference between that and a *policy method*?

Most approaches of this type of *actor-critic* methods.  What does that mean?  A state goes in, and... what comes out?

What does it mean for a policy to be stochastic?
