{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628757b2-052d-459f-a12a-2f099cd10a3d",
   "metadata": {},
   "source": [
    "# Building a Facial Recognition system from pre-trained networks\n",
    "\n",
    "You have been given extensive examples in the lab webpage, so please refer to those while completing this notebook.  Begin by installing the git repo.\n",
    "\n",
    "### Understanding your networks\n",
    "\n",
    "Take a picture with your phone of somebody in the class, and upload that picture to this notebook.  Open the image, and display it with `plt.imshow(img)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f50f847-b1bd-40f0-a4d9-4b0ab1495f63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 's3fs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01ms3fs\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NearestNeighbors\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 's3fs'"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import s3fs\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4bf523-27fd-44e8-bb84-55ab72c19e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('origProbe.jpg')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8bae21-b1c5-4bdb-a9b1-31d7db9002b2",
   "metadata": {},
   "source": [
    "Now, run it through your facial detector.  The output will be a tensor, suppose it's called `img_cropped`. Print out its shape, and then display the image. You can display it with `plt.imshow(img_cropped.permute(1,2,0))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67883ea3-6092-4c47-b301-13d9a14c0519",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn = MTCNN(keep_all=True)\n",
    "mtcnn.eval()\n",
    "cropped_img = mtcnn(img)\n",
    "print(f'shape is {cropped_img.shape}')\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(18, 3))\n",
    "\n",
    "for i in range(5):\n",
    "    axs[i].imshow(cropped_img[i].permute(1,2,0))\n",
    "    axs[i].axis('off')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c20116-620b-49e4-a734-30f568f09f50",
   "metadata": {},
   "source": [
    "Now, create an embedding of that face, and print out its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db769b-7635-40e4-a527-88c3243c3c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to('cuda')\n",
    "embeds = resnet(cropped_img.to('cuda'))\n",
    "embeds = embeds.detach().cpu().numpy()\n",
    "print(f'Shape is {embeds.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7fa883-3727-4359-999b-c873df527e9f",
   "metadata": {},
   "source": [
    "### Getting your Gallery\n",
    "\n",
    "Using the S3 bucket `s3://mlspace-data-521454461163/project/10FacialRecognition/datasets/midsfaces/`, for all midshipmen currently taking SD312, create a list of all the images as numpy arrays, and then a numpy array of all the embeddings of their cropped faces, where `embeddings[0]` is a row vector containing the embedding of `images[0]` from your list of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57b50c-af50-4f09-a6f5-04da21ce89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd312Alphas = ['M250024','M250966','M251344','M252310','M252670','M252916','M253078','M253762','M253828','M255514','M256168','M256258','M256264','M256294','M256774','M257062','M250522','M252016','M252754','M253306','M253384','M254230','M254530','M254572','M256978','M250072','M250420','M250996','M251632','M252130','M253036','M253120','M254542','M254932','M255556','M255748','M255994','M256354','M256894']\n",
    "fs = s3fs.S3FileSystem()\n",
    "all_filenames = fs.ls('s3://mlspace-data-521454461163/project/10FacialRecognition/datasets/midsfaces/')\n",
    "gallery = []\n",
    "for filename in all_filenames:\n",
    "    alpha = filename.split('/')[-1].split('.')[0]\n",
    "    if alpha in sd312Alphas:\n",
    "        with fs.open('s3://'+filename) as s3_object:\n",
    "            gallery.append(np.array(Image.open(s3_object)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1cf7ad-9aef-41c3-8814-d95255a9dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gallery_embeddings = np.zeros((len(gallery), 512))\n",
    "mtcnn = MTCNN()\n",
    "for i in range(len(gallery)):\n",
    "    cropped = mtcnn(gallery[i])\n",
    "    gallery_embeddings[i,:] = resnet(cropped.unsqueeze(0).to('cuda')).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95e63f-6c38-4cf4-8bb0-bc94cbc8c590",
   "metadata": {},
   "source": [
    "`.fit()` a sklearn `NearestNeighbor` object to your trained embeddings.  Let's keep the top-5 closest faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c008eb-87eb-4ff1-ba82-04003ba47ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=5)\n",
    "nn.fit(gallery_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70525f-f4fe-4117-8251-80441c43447f",
   "metadata": {},
   "source": [
    "### Building your recognition system\n",
    "\n",
    "Now, given your picture of someone in your class, find the five faces from SD312 which are closest in embedding space to your picture.  Write a function that displays first the probe image, and then the five gallery images closest to that probe image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f823ba10-8500-4dee-a7b5-ec95d0dee225",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = nn.kneighbors(embeds)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964ca01-3c57-4942-8731-751670e4abef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def showMatches(probe, galleries):\n",
    "    # Create a figure with 1 row and 6 columns for subplots\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(18, 3))\n",
    "\n",
    "    # Plot each image on its corresponding subplot\n",
    "    axs[0].imshow(probe)\n",
    "    axs[0].axis('off')  # Turn off axis for cleaner display\n",
    "    axs[0].set_title('Probe')\n",
    "\n",
    "    # Draw a vertical line after the first image\n",
    "    axs[0].axvline(x=0.5, color='black', linestyle='--')\n",
    "\n",
    "    for i in range(len(galleries)):\n",
    "        axs[i+1].imshow(galleries[i])\n",
    "        axs[i+1].axis('off')\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888e142-cef3-44f3-be8e-f9b940bbd057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bestGallery=[]\n",
    "for i in range(5):\n",
    "    bestGallery.append([gallery[i] for i in results[1][i]])\n",
    "    showMatches(cropped_img[0].permute(1,2,0), bestGallery[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e536069-f0a2-40ac-bbcf-74e19d76781d",
   "metadata": {},
   "source": [
    "Collect more pictures of people in the class.  Find some with people wearing a hat, or a Halloween costume, or a covid mask, or a terrible mustache.  How well do they work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3994ab-24d3-4c8f-8129-525f41fd60e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3990ce53-b1a7-4d20-bf03-c6f1030f314a",
   "metadata": {},
   "source": [
    "Expand to the full class of 2025 for your gallery images.  Then expand to all midshipmen in the dataset.  How does your accuracy change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088e7ea-175b-43e1-915d-112192e2bd12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
