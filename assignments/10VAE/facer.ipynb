{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d253a8-feec-40cb-b593-0de51c835f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn_v2\n",
    "from torchvision.models.detection import RetinaNet_ResNet50_FPN_V2_Weights\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38de50dc-2818-40dc-9a64-6427a2932e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6041dbdf-f66d-404e-a7fa-61cea2a9ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = retinanet_resnet50_fpn_v2(weights=RetinaNet_ResNet50_FPN_V2_Weights.DEFAULT)\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860201ff-b4ec-4b0f-88ec-109f236a759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/home/scs/taylor/faculty/onlySD312'\n",
    "output_dir = input_dir+'/cropped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06da70dd-e033-4676-adbd-04b43e433999",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize(size=(800,800),antialias=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2221c0-f796-494d-bb45-12c5c94d5389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[120, 122, 122,  ..., 139, 140, 140],\n",
      "         [121, 122, 123,  ..., 139, 139, 140],\n",
      "         [119, 121, 123,  ..., 139, 139, 139],\n",
      "         ...,\n",
      "         [  9,   9,   9,  ...,   5,   6,  10],\n",
      "         [  9,   9,   9,  ...,   5,   6,  10],\n",
      "         [  9,   9,   9,  ...,   5,   6,  10]],\n",
      "\n",
      "        [[149, 151, 151,  ..., 163, 164, 164],\n",
      "         [150, 151, 152,  ..., 163, 163, 164],\n",
      "         [148, 150, 152,  ..., 163, 163, 163],\n",
      "         ...,\n",
      "         [  9,   9,   9,  ...,   5,   6,  10],\n",
      "         [  9,   9,   9,  ...,   5,   6,  10],\n",
      "         [  9,   9,   9,  ...,   5,   6,  10]],\n",
      "\n",
      "        [[179, 181, 181,  ..., 189, 190, 190],\n",
      "         [180, 181, 182,  ..., 189, 189, 190],\n",
      "         [178, 180, 182,  ..., 189, 189, 189],\n",
      "         ...,\n",
      "         [  9,   9,   9,  ...,   5,   6,  10],\n",
      "         [  9,   9,   9,  ...,   5,   6,  10],\n",
      "         [  9,   9,   9,  ...,   5,   6,  10]]], dtype=torch.uint8) torch.Size([3, 320, 320])\n",
      "tensor([], size=(3, 0, 302), dtype=torch.uint8) torch.Size([3, 0, 302])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot write empty image as JPEG",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m permuted_tensor \u001b[38;5;241m=\u001b[39m cropped_face\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     20\u001b[0m normalized_tensor \u001b[38;5;241m=\u001b[39m permuted_tensor \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/torchvision/utils.py:150\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    149\u001b[0m im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(ndarr)\n\u001b[0;32m--> 150\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/PIL/Image.py:2439\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2436\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2439\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2440\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m~/anaconda3/envs/gpu2/lib/python3.10/site-packages/PIL/JpegImagePlugin.py:647\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m im\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    646\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot write empty image as JPEG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     rawmode \u001b[38;5;241m=\u001b[39m RAWMODE[im\u001b[38;5;241m.\u001b[39mmode]\n",
      "\u001b[0;31mValueError\u001b[0m: cannot write empty image as JPEG"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.jpg'):\n",
    "        image=read_image(input_dir+'/'+filename)\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(image_tensor)\n",
    "            #print(predictions)\n",
    "        for idx, prediction in enumerate(predictions[0]['boxes']):\n",
    "            box = prediction.cpu().numpy().astype(int)\n",
    "            x1, y1, x2, y2 = box\n",
    "            cropped_face = image[:, y1:y2, x1:x2]\n",
    "            print(cropped_face, cropped_face.shape)\n",
    "\n",
    "            # Save the cropped face to a file\n",
    "            output_filename = f'{os.path.splitext(filename)[0]}_face{idx+1}.jpg'\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "            permuted_tensor = cropped_face.unsqueeze(0).permute(0, 1, 2, 3)\n",
    "\n",
    "            normalized_tensor = permuted_tensor / 255\n",
    "            save_image(normalized_tensor,output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db276dd-cd72-4274-994a-7a9b999768d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
