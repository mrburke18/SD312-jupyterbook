{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e9b8dab-d40a-462f-837d-f150678e64ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Neural Nets: the Universal Approximator\n",
    "\n",
    "It has been proven that a neural net with a single infinitely-wide hidden layer, can fit any training data to an arbitrary level of accuracy.  Let's play with that a little!\n",
    "\n",
    "The following blocks create 4 datasets.  Each has the same features, namely 200 points scattered across the number line (these are stored in variable `xs`), but very different target values (stored in variables `y1`, `y2`, `y3`, and `y4`.  Those functions are then plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24386599-f3b5-4d23-84f7-a3bd8b5a0e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91cab24-7172-4c2e-9a94-a2c228d75954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs=np.linspace(-2,2,num=200)\n",
    "\n",
    "y1=2*np.cos(2*xs)\n",
    "y2=np.abs(xs)\n",
    "y3=-.5*xs*xs+xs-2\n",
    "y4=[]\n",
    "for x in xs:\n",
    "    if x<0:\n",
    "        y4.append(-2)\n",
    "    else:\n",
    "        y4.append(2)\n",
    "y4=np.array(y4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298a66e-561a-4103-a494-742e90c189ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p1=go.Scatter(x=xs,y=y1,name='cos')\n",
    "p2=go.Scatter(x=xs,y=y2,name='abs')\n",
    "p3=go.Scatter(x=xs,y=y3,name='parab')\n",
    "p4=go.Scatter(x=xs,y=y4,name='step')\n",
    "layout=go.Layout(height=500)\n",
    "fig=go.Figure(data=[p1,p2,p3,p4],layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b719fb-27a7-435d-9b67-c093b87c4d43",
   "metadata": {},
   "source": [
    "If we wanted to do linear regression on all of these, we would need to build some smart features, each hand designed for each function.  Let's just use a neural net instead.  Let's fit the cosine function first.  These next two blocks import all the necessary libraries, and then create Pytorch tensors called `data` and `targets`, which contain the x and y values we're trying to fit.\n",
    "\n",
    "The `reshape(-1,1)` function is unusual - Pytorch quite understandably expects the input and output data to be matrices of multiple features, not just single elements, so it expects a 2D array. In our toy examples, here, we need to make our data into 2d arrays. `xs.reshape(-1,1)` turns this from an array of size `(200,)` to one of size `(200,1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e50c9a-6393-4b50-9bb3-2efc7ad280e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d6ea7-6e58-4486-889c-2e6cefcdbcbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data=torch.tensor(xs.reshape(-1,1), dtype=torch.float32)\n",
    "targets=torch.tensor(y1.reshape(-1,1),dtype=torch.float32)\n",
    "print(f'data shape: {data.shape}, targets shape: {targets.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17346050-8925-49bc-a4be-d39eeb1c4905",
   "metadata": {},
   "source": [
    "OK, now let's build a Neural Net.  Use [the class notes as a guide](https://www.usna.edu/Users/cs/SD312/notes/10NNs/NeuronWeb.html), and create a net that takes in a data point of 1 feature, and consists of a single hidden layer of 3 neurons, followed by a Sigmoid activation function, followed by an output layer which outputs 1 value, with no activation.\n",
    "\n",
    "Create an instance of your object, and then print it.  What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16837555-8a94-4a89-a224-c38da07460bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6a79b6d-ca87-4aac-9f4e-150275d229a1",
   "metadata": {},
   "source": [
    "Again using the class notes as a guide, train your network using mean squared error on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd7fc0-404b-4f8c-8aea-5b3bc5a0086d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a414c8f-8eb5-413a-8a35-c38c1cb51fad",
   "metadata": {},
   "source": [
    "Next, we'd like you to create a plot showing the data you're trying to fit and your predictions.  To do this, first make a set of predictions, then call `.detach()` to extract the raw numbers from the more complex object used to track gradients and other information.  This will make a 200x1 matrix - call `.flatten()` to make a size `(200,)` vector for your plot.\n",
    "\n",
    "How close are your predictions to the true function?  Be sure to use a legend so we can tell which is the truth and which is your prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4b8d43-c046-4e3d-ac0f-bc196ddfc00b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e017e03-e4c2-4e9c-a2eb-f060e0656fa0",
   "metadata": {},
   "source": [
    "You can probably get that pretty close, right?  Now train the same network on the absolute value target (`y2`).  How close are the predictions?  Show me a plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107afe1-af36-4054-bc18-c7c14585fd47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c71636b4-4b8d-4eea-bdce-63d332804eaa",
   "metadata": {},
   "source": [
    "Now show me the other two functions (the parabola and step functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c308ae-7dff-4083-9568-037c4c0b179e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8887976-1da4-4510-955c-200b6ed92395",
   "metadata": {},
   "source": [
    "That last one, the step function, was probably the worst, right?  Well, there's a number of changes we can make.\n",
    "- We can make the hidden layer wider, by redefining the network to have a wider hidden layer.\n",
    "- We could change the [activation function](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) (ReLU is especially popular for its speed in calculating the gradient).\n",
    "- We could add more layers, with activation functions of their own.\n",
    "- We can train it longer.\n",
    "\n",
    "Try some things.  How closely can you fit it?  How low can you get the loss to go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d3987-9d1f-4c01-a604-697c4fb2735e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97bcdd00-b712-4219-802c-e256e4a44aeb",
   "metadata": {},
   "source": [
    "Now for the real trick - we can build a neural net that regresses upon multiple functions at once.  Take the neural net that worked the best on the step function above, and redefine it below so that rather than outputting a single value, it outputs 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a14ec-9d21-43cc-a459-3199ee7b9f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7149cff-0717-474b-b1ee-df9bfb493b3d",
   "metadata": {},
   "source": [
    "The below cell creates a tensor `targets` which is 200x4, where each row contains the cos of that data point, the abs of that data point, the parabola's value of that data point, and the step function of that datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9d42d-7960-4826-90a1-d7c99bd54d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alltargets=np.zeros((200,4))\n",
    "alltargets[:,0]=y1\n",
    "alltargets[:,1]=y2\n",
    "alltargets[:,2]=y3\n",
    "alltargets[:,3]=y4\n",
    "targets=torch.tensor(alltargets,dtype=torch.float32)\n",
    "print(f'targets shape is {targets.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c9c5fe-9df8-472c-bb2b-3d311deab896",
   "metadata": {},
   "source": [
    "Run your neural net, fitting your data to this 200x4 target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091ade9-8817-4b20-abb4-f9755006e03c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "558b9abb-bd7a-4c54-8b32-611fd6ab0c11",
   "metadata": {},
   "source": [
    "Make a plot showing the actual functions, and your approximations to those functions (maybe as dotted lines to make it readable).  Your graph will have 8 lines on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d2cdd9-6c91-489c-aa2c-728fa6c133ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
