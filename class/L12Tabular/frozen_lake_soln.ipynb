{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a3cd93-d0af-4823-8343-3c69948d40c6",
   "metadata": {},
   "source": [
    "# Implementing Tabular Q-Learning on a simple MDP.\n",
    "\n",
    "We will use Tabular Q-learning to learn useful behavior in a simple MDP with a small, discrete set of states and a small, discrete set of actions called \"Frozen Lake.\"  You may occasionally want to refer to [the Gymnasium documentation](https://gymnasium.farama.org/) to understand how to interact with this environment (Frozen Lake can be found under \"Environments\"->\"Toy Text\").\n",
    "\n",
    "Let's start by exploring the interface.  Below is some code that demonstrates moving through the space.  Note that later while training, you will want `render_mode` to be `None`, as rendering *dramatically* slows things down.  You should understand the code in these cells.  Play with changing the action and running it again so you can understand the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb637b-2f01-40cd-9bc0-6d2237b85c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import numpy.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b421f3-b5b8-41ff-a0f7-aaf23eb58b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='ansi')\n",
    "observation, info = env.reset()\n",
    "print(f'current state is {observation}, board looks like:')\n",
    "print(env.render())\n",
    "a = 1\n",
    "observation, reward, terminated, truncated, info = env.step(a)\n",
    "print(f'Taking action {a}. Now in state {observation} and received reward {reward}.')\n",
    "print(f'terminated is {terminated}, truncated is {truncated}')\n",
    "print(f'Board looks like:')\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e60513-2dc6-45fe-a25d-1ccbd03d7712",
   "metadata": {},
   "source": [
    "Now, write code which hardcodes in a sequence of actions that moves the agent all the way to the goal without stepping in any holes.  Print out the render and reward after each step, so you can observe the reward is 1 when you reach the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b642dc95-f920-4b33-8c2c-d13426eb244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [1, 1, 2, 1, 2, 2]\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode='ansi')\n",
    "env.reset()\n",
    "env.render()\n",
    "for a in actions:\n",
    "    observation, reward, terminated, truncated, info = env.step(a)\n",
    "    print(reward)\n",
    "    print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d200ec-e55f-446c-984d-4906a0b3585b",
   "metadata": {},
   "source": [
    "Now we understand the interface (and can look up what we don't), so it's time to actually do some RL. For our first version of this problem, we're going to make `is_slippery=False`, and `map_name=\"4x4\"`, so we're working with the simplest version of this problem.  For this problem, make the discount factor $\\gamma=.9$.\n",
    "\n",
    "You'll need to do the following steps:\n",
    "\n",
    "- Create a numpy array of zeros to be your Q-values, which is `number of states x number of actions`.\n",
    "- Implement $\\epsilon$-greedy exploration.  After doing this, pause before implementing the next bullet. Prove to yourself that your agent is visiting a variety of states during the course of this exploration.\n",
    "- To that exploration loop, add tabular Q-learning.\n",
    "\n",
    "Eventually, your agent will consistently reach the goal on its own.  In an additional cell below your training code, write code that uses a greedy strategy (always uses the most-valuable action from your learned Q-values), and renders the environment after every step, to demonstrate that it quickly reaches the goal. Your loop should run until the environment is terminated or truncated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6dc3d-4d70-4f62-ae29-011d66143987",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_slippery = False\n",
    "success_count = []\n",
    "step_count = []\n",
    "\n",
    "state_counts = dict()\n",
    "for i in range(16):\n",
    "    state_counts[i] = 0\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=is_slippery)\n",
    "Qs = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "observation, info = env.reset()\n",
    "for i in range(100001):\n",
    "    old_observation = observation\n",
    "    if np.random.random() < EPSILON:\n",
    "        action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    else:\n",
    "        action = np.argmax(Qs[observation,:])\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    Qs[old_observation, action] = ALPHA * (reward + GAMMA*np.max(Qs[observation,:])) + (1-ALPHA)*Qs[old_observation,action]\n",
    "    \n",
    "    state_counts[observation] += 1\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9248e-85d2-41c7-8fa5-3f7ac5087446",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=is_slippery, render_mode='ansi')\n",
    "observation, info = env.reset()\n",
    "print(env.render())\n",
    "terminated = truncated = False\n",
    "while not (terminated or truncated):\n",
    "    action = np.argmax(Qs[observation, :])\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b6db80-3d92-4b27-895a-ec7f23c08888",
   "metadata": {},
   "source": [
    "Do it again, but make the lake 8x8.  How much more training does it seem you need to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa69994-bb3d-420b-94c2-c4f3fa004bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0731ad63-7fac-4f45-8fb4-8cd98bfcd626",
   "metadata": {},
   "source": [
    "Now, we're going to make it slippery.  For non-slippery lakes, it makes sense for your learning rate to be quite high.  For slippery lakes, we will want a smaller learning rate.  Before coding, write in a markdown cell below why that would be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceace0a-935e-44bd-8694-9fcabc5156ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a04f87-97ba-484d-b645-0dd95815db08",
   "metadata": {},
   "source": [
    "Below put your code training and demoing your slippery environments.  There is no policy that guarantees success in reaching the goal on a slippery lake.  To demonstrate success, rather than doing a single run, keep rendering off, and run it 100 times to termination. In what percentage of the times does your agent reach the goal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57527e99-6ae7-4c0f-94d3-6ac9f6dfaae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
