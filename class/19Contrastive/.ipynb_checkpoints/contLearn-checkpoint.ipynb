{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf173e46-4e34-4728-8917-3c994a35b270",
   "metadata": {},
   "source": [
    "# Training a facial recognition system with Contrastive Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c030b7c7-2d5a-43f8-98b0-383222264135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision.models import resnet18,ResNet18_Weights\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "from glob import glob\n",
    "import random\n",
    "from torchvision.io import read_image, ImageReadMode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b82af5-ed81-4681-8e70-1f03c3b715b2",
   "metadata": {},
   "source": [
    "Here I just get a ResNet18 model, and replace its classification layer with an output layer that outputs a 256-dimensional vector.  That will be the size of my embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590a148d-acee-41d9-a35a-982e4cfbc1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "num_feats = model.fc.in_features\n",
    "model.fc = nn.Linear(num_feats, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c7476b-27ac-42fb-8f49-6a0a0eb9fcef",
   "metadata": {},
   "source": [
    "Here's my dataset.  A lot actually is happening here.  The key thing to understand here is that it returns an *anchor* image, a *positive* image of the same person, and a *negative* image of a different person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55df300-853f-4382-aff6-bdeddb9e4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContLearnDataset(Dataset):\n",
    "    def __init__(self, dir, transforms=None):\n",
    "        self.dir = dir\n",
    "        self.transforms = transforms\n",
    "        self.filenames = glob(dir+'/*/*.png')\n",
    "        self.ids = set([ self.path2id(pth) for pth in self.filenames ])\n",
    "\n",
    "    def path2id(self, path):\n",
    "        return path.split('/')[-2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor = self.filenames[idx]\n",
    "\n",
    "        id = self.path2id(anchor)\n",
    "        \n",
    "        poscands = [fn for fn in self.filenames if id in fn and fn != anchor]\n",
    "        ind = torch.randint(len(poscands),(1,))[0]\n",
    "        positive = poscands[ind]\n",
    "        \n",
    "        negcands = [fn for fn in self.filenames if id not in fn]\n",
    "        ind = torch.randint(len(negcands),(1,))[0]\n",
    "        negative = negcands[ind]\n",
    "\n",
    "        anchor = read_image(anchor,mode=ImageReadMode.RGB)\n",
    "        positive = read_image(positive,mode=ImageReadMode.RGB)\n",
    "        negative = read_image(negative,mode=ImageReadMode.RGB)\n",
    "        if self.transforms is not None:\n",
    "            return self.transforms(anchor), self.transforms(positive), self.transforms(negative)\n",
    "        return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb264b-342c-4c47-bf4a-fc03b91f9c68",
   "metadata": {},
   "source": [
    "I build my datasets and my dataloader..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5559d12-5b91-4236-9254-0f3ba2d71f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "cld = ContLearnDataset('data/faces/training/',transforms=transform)\n",
    "cld2 = ContLearnDataset('data/faces/testing/',transforms=transform)\n",
    "\n",
    "dl = DataLoader(cld, batch_size=64, num_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0932a-2fb8-4483-8320-9e70724b4e01",
   "metadata": {},
   "source": [
    "I train using TripletMarginLoss, to encourage the embeddings of the same person to be close to each other, and the embeddings of different people to be farther away from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22508d85-d364-40ec-9f88-875c98636fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                          | 1/201 [00:00<03:07,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.775298625230789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████▌                    | 101/201 [01:00<00:59,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.024434760212898254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 201/201 [02:00<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1770007610321045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model=model.to('cuda')\n",
    "EPOCHS = 201\n",
    "\n",
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=.001)\n",
    "\n",
    "for epoch in tqdm.tqdm(range(EPOCHS)):\n",
    "    totalloss=0\n",
    "    for batch, (a, p, n) in enumerate(dl):\n",
    "        a,p,n = a.to('cuda'), p.to('cuda'), n.to('cuda')\n",
    "        aem = model(a)\n",
    "        pem = model(p)\n",
    "        nem = model(n)\n",
    "        loss = criterion(aem, pem, nem)\n",
    "\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        totalloss+=loss.item()\n",
    "    if epoch%100==0:\n",
    "        print(totalloss)\n",
    "torch.save(model, 'fr.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03247ee0-85c7-4426-92f8-04d238f881a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('fr.pt')\n",
    "model=model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31178565-721e-4d22-90a5-81542c16234a",
   "metadata": {},
   "source": [
    "Here I test on a testing set, which was not trained on.  `a,p,n` are an anchor, positive, and negative examples from a different dataset of people.  You can see the distance between the anchor and positive example is far smaller than the distances between the anchor and negative, and positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1cf2f1-6fad-4a89-b966-186ab38fc009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256])\n",
      "a/p: 0.007957154884934425\n",
      "a/n: 0.5905918478965759\n",
      "p/n: 0.5849518775939941\n"
     ]
    }
   ],
   "source": [
    "a,p,n = cld2[20]\n",
    "a,p,n = a.reshape((1,3,112,92)), p.reshape((1,3,112,92)), n.reshape((1,3,112,92))\n",
    "vals=torch.cat((a,p,n),dim=0).to('cuda')\n",
    "res=model(vals).detach()\n",
    "print(res.shape)\n",
    "res[0].shape\n",
    "print(f'a/p: {nn.functional.mse_loss(res[0],res[1])}')\n",
    "print(f'a/n: {nn.functional.mse_loss(res[0],res[2])}')\n",
    "print(f'p/n: {nn.functional.mse_loss(res[1],res[2])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f0a8b-0199-4391-bf1f-5956d5619b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
